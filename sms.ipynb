{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for 1 database \n",
    "df1 = pd.read_csv(\"data/emails.csv\", encoding='latin-1')\n",
    "df1.columns = ['text', 'label']\n",
    "df1['label'] = df1['label'].replace([1],'spam')\n",
    "df1['label'] = df1['label'].replace([0],'ham')\n",
    "\n",
    "# Code for 2 database\n",
    "df2 = pd.read_csv(\"data/spam_ham_dataset.csv\", encoding='latin-1')\n",
    "df2.drop(['label_num', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "df2 = df2[['text', 'label']]\n",
    "\n",
    "# Code for 3 database\n",
    "df3 = pd.read_csv(\"data/spam.csv\", encoding='latin-1')\n",
    "df3.dropna(how=\"any\", inplace=True, axis=1)\n",
    "df3.columns = ['label', 'text']\n",
    "df3 = df3[['text', 'label']]\n",
    "\n",
    "# Calculate text\n",
    "df1['text_length'] = df1['text'].apply(lambda x: len(x))\n",
    "df2['text_length'] = df2['text'].apply(lambda x: len(x))\n",
    "df3['text_length'] = df3['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING DANYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first method\n",
    "from gensim.utils import simple_preprocess\n",
    "df1['tokens_pp1'] = df1['text'].apply(lambda x: simple_preprocess(x, min_len=2, max_len=15))\n",
    "df2['tokens_pp1'] = df2['text'].apply(lambda x: simple_preprocess(x, min_len=2, max_len=15))\n",
    "df3['tokens_pp1'] = df3['text'].apply(lambda x: simple_preprocess(x, min_len=2, max_len=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second method\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.disable_pipe(\"ner\")\n",
    "nlp.disable_pipe(\"tok2vec\")\n",
    "def spacy_tokenizer(document):\n",
    "    tokens = nlp(document)\n",
    "    tokens = [token.lemma_ for token in tokens if (\n",
    "        token.is_stop == False and \\\n",
    "        token.is_punct == False and \\\n",
    "        token.lemma_.strip()!= '')]\n",
    "    return tokens\n",
    "\n",
    "df1['tokens_pp2'] = df1['text'].apply(spacy_tokenizer)\n",
    "print(\"done1\")\n",
    "df2['tokens_pp2'] = df2['text'].apply(spacy_tokenizer)\n",
    "print(\"done2\")\n",
    "df3['tokens_pp2'] = df3['text'].apply(spacy_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third method\n",
    "def remove_punctuation(text):\n",
    "    no_punct=\"\".join([words for words in text if words not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword.add('Subject')\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopword])\n",
    "\n",
    "def tokenize(text):\n",
    "    split=re.split(\"\\W+\",text) \n",
    "    return split\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df1['tokens_pp3'] = df1['text'].apply(remove_punctuation)\n",
    "df1['tokens_pp3'] = df1['tokens_pp3'].apply(remove_stopwords)\n",
    "df1['tokens_pp3'] = df1['tokens_pp3'].apply(tokenize)\n",
    "df1['tokens_pp3'] = df1['tokens_pp3'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "df2['tokens_pp3'] = df2['text'].apply(remove_punctuation)\n",
    "df2['tokens_pp3'] = df2['tokens_pp3'].apply(remove_stopwords)\n",
    "df2['tokens_pp3'] = df2['tokens_pp3'].apply(tokenize)\n",
    "df2['tokens_pp3'] = df2['tokens_pp3'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "df3['tokens_pp3'] = df3['text'].apply(remove_punctuation)\n",
    "df3['tokens_pp3'] = df3['tokens_pp3'].apply(remove_stopwords)\n",
    "df3['tokens_pp3'] = df3['tokens_pp3'].apply(tokenize)\n",
    "df3['tokens_pp3'] = df3['tokens_pp3'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first method TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_pca_features = 100\n",
    "\n",
    "tfidf_result = []\n",
    "i = 0\n",
    "# for df in [df1,df2,df3]:\n",
    "for df in [df3]:\n",
    "  for x in [df[\"tokens_pp1\"],df[\"tokens_pp2\"],df[\"tokens_pp3\"]]:\n",
    "      print(\"Iteration number\",i)\n",
    "      tfidf_vector = TfidfVectorizer(input=\"content\", analyzer=\"word\")\n",
    "      result = tfidf_vector.fit_transform(x.apply(lambda x: ' '.join(x)))\n",
    "      dense = result.todense()\n",
    "      denselist = dense.tolist()\n",
    "      df_proc = pd.DataFrame(denselist,columns=tfidf_vector.get_feature_names_out())\n",
    "      scalar = StandardScaler()\n",
    "      scalar.fit(df_proc)\n",
    "      scaled_data = scalar.transform(df_proc)\n",
    "      pca = PCA(n_components = n_pca_features)\n",
    "      pca.fit(scaled_data)\n",
    "      x_pca = pca.transform(scaled_data)\n",
    "      tmp = np.array(df['text_length'])\n",
    "      features = np.concatenate((x_pca, tmp[:,np.newaxis]), axis=1)\n",
    "      tfidf_result.append(features)\n",
    "      i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second method Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(doc):\n",
    "  return doc\n",
    "\n",
    "bow_result = []\n",
    "i = 0\n",
    "for df in [df1, df2, df3]:\n",
    "  for x in [df[\"tokens_pp1\"],df[\"tokens_pp2\"],df[\"tokens_pp3\"]]:\n",
    "    print(f\"Iteration number: {i}\")\n",
    "    cv = CountVectorizer(tokenizer=dummy, preprocessor=dummy,)\n",
    "    counts = cv.fit_transform(x)\n",
    "    print(\"check\")\n",
    "    dense = counts.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_proc = pd.DataFrame(denselist,columns=cv.get_feature_names())\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(df_proc)\n",
    "    scaled_data = scalar.transform(df_proc)\n",
    "    pca = PCA(n_components = n_pca_features)\n",
    "    pca.fit(scaled_data)\n",
    "    x_pca = pca.transform(scaled_data)\n",
    "    tmp = np.array(df['text_length'])\n",
    "    features = np.concatenate((x_pca, tmp[:,np.newaxis]), axis=1)\n",
    "    bow_result.append(features)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third method word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "n_pca_features = 20\n",
    "vector_size=100\n",
    "def word2vec(tokens):\n",
    "    if len(tokens) > 0:\n",
    "        word2vec = model.wv[tokens]\n",
    "        return np.average(word2vec, axis=0)\n",
    "    else:\n",
    "        # When there is no token in tokens\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "word2vec_result = []\n",
    "df1 = df1.sample(frac=1).reset_index(drop=True)\n",
    "dfSUM = pd.concat([df1.iloc[1000:,:], df3], axis=0,ignore_index=True)\n",
    "for df in [dfSUM]:\n",
    "  for x in [df[\"tokens_pp3\"]]:\n",
    "    model = Word2Vec(\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4,\n",
    "        vector_size=vector_size\n",
    "    )\n",
    "    model.build_vocab(x)\n",
    "    model.train(x, total_examples=model.corpus_count, epochs=100)\n",
    "    \n",
    "    result = x.apply(word2vec)\n",
    "    pca = PCA(n_components=n_pca_features)\n",
    "    x_pca = pca.fit_transform(pd.DataFrame.from_records(result))\n",
    "    tmp = np.array(df['text_length'])\n",
    "    features = np.concatenate((x_pca, tmp[:,np.newaxis]), axis=1)\n",
    "    word2vec_result.append(features)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PÄ˜TLA EKSPERYMENTALNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "i = 0\n",
    "metrics_of_tfidf = []\n",
    "# for df in [df1, df2, df3]:\n",
    "for df in [df3]:\n",
    "    y = np.array(df['label'])\n",
    "    # print(y)\n",
    "    crossvalid_scores = []\n",
    "    for X in tfidf_result[3*i:3*i+3]:\n",
    "        rkf = RepeatedKFold(n_splits=5, n_repeats=2,random_state=1234)\n",
    "        accuracies = []\n",
    "        for train_index, test_index in rkf.split(X):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            gnb = GaussianNB()\n",
    "            y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "            accuracies.append(accuracy_score(y_pred, y_test))\n",
    "        crossvalid_scores.append(accuracies)\n",
    "    metrics_of_tfidf.append(crossvalid_scores)    \n",
    "    i+=1\n",
    "\n",
    "print((np.shape(metrics_of_tfidf)))\n",
    "print(((metrics_of_tfidf)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "i = 0\n",
    "metrics_of_bagofwords = []\n",
    "for df in [df1, df2, df3]:\n",
    "    y = df['label']\n",
    "    # print(y)\n",
    "    crossvalid_scores = []\n",
    "    for X in tfidf_result[3*i:3*i+3]:\n",
    "        rkf = RepeatedKFold(n_splits=5, n_repeats=2,random_state=1234)\n",
    "        accuracies = []\n",
    "        for train_index, test_index in rkf.split(X):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], y[test_index]\n",
    "            y_train, y_test = X[train_index], y[test_index]\n",
    "            gnb = GaussianNB()\n",
    "            y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "            accuracies.append(accuracy_score(y_pred, y_test))\n",
    "        crossvalid_scores.append(accuracies)\n",
    "    metrics_of_bagofwords.append(crossvalid_scores)    \n",
    "    i+=1\n",
    "\n",
    "print((np.shape(metrics_of_bagofwords)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "i = 0\n",
    "metrics_of_word2vec = []\n",
    "for df in [df1]:\n",
    "    y = np.array(df['label'])\n",
    "    # print(y)\n",
    "    crossvalid_scores = []\n",
    "    for X in word2vec_result[3*i:3*i+3]:\n",
    "        # print(X)\n",
    "        rkf = RepeatedKFold(n_splits=5, n_repeats=2,random_state=1234)\n",
    "        accuracies = []\n",
    "        for train_index, test_index in rkf.split(X):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            gnb = GaussianNB()\n",
    "            y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "            accuracies.append(accuracy_score(y_pred, y_test))\n",
    "        crossvalid_scores.append(accuracies)\n",
    "    metrics_of_word2vec.append(crossvalid_scores)    \n",
    "    i+=1\n",
    "\n",
    "\n",
    "print((np.shape(metrics_of_word2vec)))\n",
    "print(((metrics_of_word2vec)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST D1 NA WYTRENOWANYM D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df3]:\n",
    "    y = np.array(df['label'])\n",
    "    for X in word2vec_result:\n",
    "      gnb = GaussianNB()\n",
    "      gnb.fit(X, y)\n",
    "      \n",
    "#test\n",
    "for df in [df1]:\n",
    "  for x in [df[\"tokens_pp3\"]]:\n",
    "    model.build_vocab(x,update=True)\n",
    "    result = x.apply(word2vec)\n",
    "    pca = PCA(n_components=n_pca_features)\n",
    "    x_pca = pca.fit_transform(pd.DataFrame.from_records(result))\n",
    "    tmp = np.array(df['text_length'])\n",
    "    features = np.concatenate((x_pca, tmp[:,np.newaxis]), axis=1)\n",
    "    switch_test = features\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "for df in [df1]:\n",
    "    y = np.array(df['label'])\n",
    "    X = switch_test\n",
    "    y_pred = gnb.predict(X)\n",
    "    print(accuracy_score(y_pred,y))\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST D3 NA D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df2]:\n",
    "    y = np.array(df['label'])\n",
    "    for X in word2vec_result:\n",
    "      gnb = GaussianNB()\n",
    "      gnb.fit(X, y)\n",
    "      \n",
    "#test\n",
    "for df in [df3]:\n",
    "  for x in [df[\"tokens_pp3\"]]:\n",
    "    model.build_vocab(x,update=True)\n",
    "    result = x.apply(word2vec)\n",
    "    pca = PCA(n_components=n_pca_features)\n",
    "    x_pca = pca.fit_transform(pd.DataFrame.from_records(result))\n",
    "    tmp = np.array(df['text_length'])\n",
    "    features = np.concatenate((x_pca, tmp[:,np.newaxis]), axis=1)\n",
    "    switch_test = features\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "for df in [df3]:\n",
    "    y = np.array(df['label'])\n",
    "    X = switch_test\n",
    "    y_pred = gnb.predict(X)\n",
    "    print(accuracy_score(y_pred,y))\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST MIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [dfSUM]:\n",
    "    # print(df)\n",
    "    y = np.array(df['label'])\n",
    "    for X in word2vec_result:\n",
    "      gnb = GaussianNB()\n",
    "      print(y)\n",
    "      gnb.fit(X, y)\n",
    "      \n",
    "#test\n",
    "\n",
    "for df in [df1.iloc[:1000,:]]:\n",
    "  for x in [df[\"tokens_pp3\"]]:\n",
    "    model.build_vocab(x,update=True)\n",
    "    result = x.apply(word2vec)\n",
    "    pca = PCA(n_components=n_pca_features)\n",
    "    x_pca = pca.fit_transform(pd.DataFrame.from_records(result))\n",
    "    tmp = np.array(df['text_length'])\n",
    "    features = np.concatenate((x_pca, tmp[:,np.newaxis]), axis=1)\n",
    "    switch_test = features\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "for df in [df1.iloc[:1000,:]]:\n",
    "    y = np.array(df['label'])\n",
    "    X = switch_test\n",
    "    y_pred = gnb.predict(X)\n",
    "    print(accuracy_score(y_pred,y))\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "alfa = .05\n",
    "n_items = 9\n",
    "t_statistic = np.zeros((n_items,n_items))\n",
    "p_value = np.zeros((n_items,n_items))\n",
    "\n",
    "# scores_d1 = [*metrics_of_tfidf[0], *metrics_of_bagofwords[0], *metrics_of_word2vec[0]]\n",
    "# scores_d2 = [*metrics_of_tfidf[1], *metrics_of_bagofwords[1], *metrics_of_word2vec[1]]\n",
    "# scores_d3 = [*metrics_of_tfidf[2], *metrics_of_bagofwords[2], *metrics_of_word2vec[2]]\n",
    "scores_d1 = [\n",
    "    *[[0.7574171029668412, 0.7513089005235603, 0.7696335078534031, 0.7694323144104803, 0.7720524017467248, 0.7591623036649214, 0.27137870855148344, 0.7713787085514834, 0.7563318777292577, 0.765938864628821], [0.7705061082024433, 0.7504363001745201, 0.7766143106457243, 0.7703056768558952, 0.7816593886462883, 0.7591623036649214, 0.7731239092495636, 0.7757417102966842, 0.7580786026200873, 0.7720524017467248], [0.768760907504363, 0.7591623036649214, 0.7705061082024433, 0.7650655021834061, 0.7039301310043669, 0.7556719022687609, 0.7801047120418848, 0.7766143106457243, 0.7615720524017467, 0.7554585152838428]],\n",
    "    *[[0.8708551483420593, 0.8621291448516579, 0.87521815008726, 0.8558951965065502, 0.8655021834061135, 0.8542757417102966, 0.8717277486910995, 0.8787085514834206, 0.8655021834061135, 0.8541484716157205], [0.9048865619546248, 0.918848167539267, 0.9179755671902269, 0.8995633187772926, 0.9039301310043668, 0.8996509598603839, 0.9179755671902269, 0.9153577661431065, 0.9126637554585153, 0.9082969432314411], [0.8996509598603839, 0.8979057591623036, 0.912739965095986, 0.8873362445414847, 0.8873362445414847, 0.887434554973822, 0.9083769633507853, 0.9040139616055847, 0.891703056768559, 0.8960698689956332]],\n",
    "    *[[0.7626527050610821, 0.7556719022687609, 0.7670157068062827, 0.7668122270742358, 0.7746724890829695, 0.7556719022687609, 0.7722513089005235, 0.7705061082024433, 0.7510917030567685, 0.7729257641921398], [0.7609075043630017, 0.7547993019197208, 0.7731239092495636, 0.7685589519650655, 0.7746724890829695, 0.7591623036649214, 0.7722513089005235, 0.7731239092495636, 0.7572052401746725, 0.77117903930131], [0.7556719022687609, 0.7102966841186736, 0.7722513089005235, 0.7580786026200873, 0.7737991266375546, 0.7556719022687609, 0.7661431064572426, 0.7696335078534031, 0.7554585152838428, 0.7668122270742358]]\n",
    "]\n",
    "scores_d2 = [\n",
    "    *[[0.7391304347826086, 0.7659574468085106, 0.7543520309477756, 0.7340425531914894, 0.7214700193423598, 0.7352657004830918, 0.7398452611218569, 0.7601547388781431, 0.7330754352030948, 0.7437137330754352], [0.7381642512077294, 0.7620889748549323, 0.758220502901354, 0.7311411992263056, 0.7214700193423598, 0.740096618357488, 0.7398452611218569, 0.7572533849129593, 0.7292069632495164, 0.741779497098646], [0.7439613526570048, 0.7678916827852998, 0.7543520309477756, 0.7330754352030948, 0.7272727272727273, 0.7333333333333333, 0.7446808510638298, 0.7620889748549323, 0.7408123791102514, 0.7437137330754352]],\n",
    "    *[[0.9265700483091788, 0.9313346228239845, 0.925531914893617, 0.9294003868471954, 0.9081237911025145, 0.9246376811594202, 0.9177949709864603, 0.9342359767891683, 0.9235976789168279, 0.9081237911025145], [0.9207729468599034, 0.9235976789168279, 0.9235976789168279, 0.9497098646034816, 0.9352030947775629, 0.9468599033816425, 0.913926499032882, 0.9506769825918762, 0.9313346228239845, 0.9168278529980658], [0.9314009661835749, 0.9235976789168279, 0.9284332688588007, 0.9448742746615088, 0.9439071566731141, 0.9439613526570049, 0.9177949709864603, 0.9516441005802708, 0.9323017408123792, 0.9216634429400387]],\n",
    "    *[[0.7516908212560387, 0.7678916827852998, 0.758220502901354, 0.7330754352030948, 0.7388781431334622, 0.7458937198067633, 0.7475822050290135, 0.7630560928433269, 0.7427466150870407, 0.7495164410058027], [0.744927536231884, 0.7611218568665378, 0.7601547388781431, 0.730174081237911, 0.7359767891682786, 0.7478260869565218, 0.7456479690522244, 0.7591876208897486, 0.7350096711798839, 0.7485493230174082], [0.7391304347826086, 0.758220502901354, 0.7524177949709865, 0.7253384912959381, 0.723404255319149, 0.7391304347826086, 0.7379110251450677, 0.7562862669245648, 0.7330754352030948, 0.7427466150870407]]\n",
    "]\n",
    "scores_d3 = [\n",
    "    *[[0.20269058295964126, 0.17399103139013453, 0.19658886894075403, 0.20287253141831238, 0.18312387791741472, 0.21614349775784752, 0.18744394618834082, 0.19389587073608616, 0.1741472172351885, 0.19210053859964094], [0.18475336322869956, 0.14887892376681613, 0.21364452423698385, 0.17594254937163376, 0.1732495511669659, 0.6026905829596413, 0.15874439461883408, 0.1822262118491921, 0.16427289048473967, 0.16786355475763015], [0.17847533632286997, 0.14349775784753363, 0.16427289048473967, 0.16337522441651706, 0.15978456014362658, 0.4439461883408072, 0.15336322869955157, 0.17145421903052063, 0.1526032315978456, 0.15978456014362658]],\n",
    "    *[[0.9865470852017937, 0.979372197309417, 0.9829443447037702, 0.9793536804308797, 0.9820466786355476, 0.9838565022421525, 0.9865470852017937, 0.9784560143626571, 0.9829443447037702, 0.9793536804308797], [0.9748878923766816, 0.9757847533632287, 0.9784560143626571, 0.9766606822262118, 0.9784560143626571, 0.9730941704035875, 0.9757847533632287, 0.9766606822262118, 0.9802513464991023, 0.9820466786355476], [0.9766816143497757, 0.9739910313901345, 0.9847396768402155, 0.9802513464991023, 0.9775583482944344, 0.9748878923766816, 0.9739910313901345, 0.9766606822262118, 0.9847396768402155, 0.9793536804308797]],\n",
    "    *[[0.20448430493273542, 0.17668161434977578, 0.2001795332136445, 0.20466786355475763, 0.18940754039497307, 0.21524663677130046, 0.19103139013452916, 0.20915619389587073, 0.17863554757630162, 0.19030520646319568], [0.18923766816143497, 0.16412556053811658, 0.1867145421903052, 0.1867145421903052, 0.16786355475763015, 0.21076233183856502, 0.18026905829596412, 0.19030520646319568, 0.1561938958707361, 0.17145421903052063], [0.19641255605381167, 0.16233183856502242, 0.38420107719928187, 0.1813285457809695, 0.1947935368043088, 0.4591928251121076, 0.18026905829596412, 0.1947935368043088, 0.15888689407540396, 0.17953321364452424]]\n",
    "]\n",
    "\n",
    "\n",
    "scr = scores_d3\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(edgeitems=30,linewidth=100000)\n",
    "for i in range(n_items):\n",
    "    for j in range(n_items):\n",
    "        t_statistic[i, j], p_value[i, j] = ttest_rel(scr[i], scr[j])\n",
    "print(\"t-statistic:\\n\", t_statistic, \"\\n\\np-value:\\n\", p_value)\n",
    "\n",
    "from tabulate import tabulate\n",
    "headers = [\"P1_TF\",\"P2_TF\",\"P3_TF\",\"P1_W2V\",\"P2_W2V\",\"P3_W2V\",\"P1_BOW\",\"P2_BOW\",\"P3_BOW\"]\n",
    "names_column = np.array([[\"P1_TF\"],[\"P2_TF\"],[\"P3_TF\"],[\"P1_W2V\"],[\"P2_W2V\"],[\"P3_W2V\"],[\"P1_BOW\"],[\"P2_BOW\"],[\"P3_BOW\"]])\n",
    "advantage = np.zeros((9,9))\n",
    "advantage[t_statistic > 0] = 1\n",
    "advantage_table = tabulate(np.concatenate(\n",
    "    (names_column, advantage), axis=1), headers)\n",
    "# print(\"Advantage:\\n\", advantage_table)\n",
    "significance = np.zeros((9,9))\n",
    "significance[p_value <= alfa] = 1\n",
    "significance_table = tabulate(np.concatenate(\n",
    "    (names_column, significance), axis=1), headers)\n",
    "# print(\"Statistical significance (alpha = 0.05):\\n\", significance_table)\n",
    "stat_better = significance * advantage\n",
    "stat_better_table = tabulate(np.concatenate(\n",
    "    (names_column, stat_better), axis=1), headers)\n",
    "print(\"Statistically significantly better:\\n\", stat_better_table)\n",
    "i = 0\n",
    "for s in scr:\n",
    "    print(headers[i]+' -> {:.2f} ({:.2f})'.format(np.mean(s),np.std(s)))\n",
    "    i +=1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93c75f0a19aae10834c127a0e76b6dd6efcb4fb40b4cb9de735e398e527f223f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
